{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999be40e-853c-44c9-bf14-f2cd91075f0d",
   "metadata": {},
   "source": [
    "## Fine tuning LLM with Prompt fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457ae61-8881-4112-8a01-de33fb4834b2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef2a3b0-3eee-4461-871c-54a647b3233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorWithPadding,\n",
    "    AdamW,\n",
    "    get_scheduler)\n",
    "\n",
    "from peft import (get_peft_config, \n",
    "get_peft_model, \n",
    "PromptTuningInit, \n",
    "PromptTuningConfig, \n",
    "TaskType, \n",
    "PeftType,\n",
    "PeftModel,\n",
    "PeftConfig)\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pynvml import *\n",
    "import subprocess as sp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2161fea7-ed47-43a7-92b5-47db1521f3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45090c5b-c8f2-4c72-98a0-9f35346cf1fe",
   "metadata": {},
   "source": [
    "#### Set deivce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9b0308-3a35-4e60-8e16-7d0f2a6b2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a2dc12-7c72-4686-a65e-7e44a1812dc6",
   "metadata": {},
   "source": [
    "#### GPU Utilization stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e285d9-6a28-455b-af01-caffdabe08f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11441 MiB, 3 MiB']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print summary statistics for the GPU utilization and the training run\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.total,memory.used --format=csv\"\n",
    "    memory_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    return memory_info\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7841a-b3af-41ad-980b-d32065bed6d3",
   "metadata": {},
   "source": [
    "#### Initialize constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89b21539-9b20-4448-8691-4038a85ce2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values\n",
    "dataset_name = 'gsm8k'\n",
    "data_split = 'main'\n",
    "base_model = \"google/flan-t5-large\"\n",
    "task_type = TaskType.SEQ_2_SEQ_LM\n",
    "prompt_tuning_init = PromptTuningInit.TEXT\n",
    "num_virtual_tokens = 10\n",
    "prompt = \"Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan to solve the problem step by step. Then, let's answer the question step by step (pay attention to commonsense and logical coherence).\"\n",
    "max_target_length = 100\n",
    "lr=1e-4\n",
    "num_epochs = 3\n",
    "max_length_output=300\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 4\n",
    "test_batch_size = 4\n",
    "trained_model_path = \"model_ckpts/flan/large-Peft-Eval_306\"\n",
    "test_responses_path = \"large_peft_eval_306\"\n",
    "num_warmup_steps=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9cfd1-6c9d-48e3-8254-637f8be51bd4",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd6bd9d-391c-452e-988c-56d2b2ee22a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 7473\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1319\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = load_dataset(dataset_name, data_split)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fcbca-ed45-4d5d-8323-1151b9c91714",
   "metadata": {},
   "source": [
    "#### Split Data - Train, Eval, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f475ba4-ec62-4434-8207-e7d63874f900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 7473\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 659\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 660\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split test into test and eval\n",
    "test_valid = data['test'].train_test_split(test_size=0.5)\n",
    "test_valid['train']\n",
    "\n",
    "data = DatasetDict({\n",
    "    'train': data['train'],\n",
    "    'eval': test_valid['train'],\n",
    "    'test': test_valid['test']\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a3fa5-c319-4d25-b7e9-586780928ba7",
   "metadata": {},
   "source": [
    "#### Training LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cfb02c5-00c6-40a3-ad22-3f5a6b71b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "def preprocessDataForPromptFT(data):\n",
    "    batch_size = len(data[\"question\"])\n",
    "    questions = [f\"Question : {question}\" for question in data[\"question\"]]\n",
    "     # Extract the steps and the final answer\n",
    "    answers = []\n",
    "    for elem in data[\"answer\"]:\n",
    "        steps, final_answer = elem.split(\"####\")\n",
    "        final_answer = final_answer.strip(\" \")\n",
    "        steps = re.sub(r'<<.*?>>', '', steps)\n",
    "        answers.append(\"Steps: \" + steps.replace(\"\\n\", \"\") + \" Answer: \" + final_answer)\n",
    "    # Tokenize input text and labels\n",
    "    model_inputs = tokenizer(questions)\n",
    "    outputs = tokenizer(answers)\n",
    "    # Pad the labels\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        output_input_ids = outputs[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # concatenate the input ids of question and the steps \n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + output_input_ids\n",
    "        # Replace padding token id's of the labels by -100 so it's not taken into consideration by the loss\n",
    "        outputs[\"input_ids\"][i] = [-100] * len(sample_input_ids) + output_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        output_input_ids = outputs[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_target_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_target_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        outputs[\"input_ids\"][i] = [-100] * (max_target_length - len(sample_input_ids)) + output_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_target_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_target_length])\n",
    "        outputs[\"input_ids\"][i] = torch.tensor(outputs[\"input_ids\"][i][:max_target_length])\n",
    "    model_inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1904692-0d19-4584-a9e5-f7988bd84a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a188b5-0be3-4861-93e8-cdf6d0e9a6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0bfa1b5a234ffe97a6026e0437c035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0adf1d2a6d426e8ef597637eca4bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/659 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a6bd30ecff48b99c40e2a7925ef0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7473\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 659\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 660\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = data.map(preprocessDataForPromptFT, batched = True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e72b3b8-b6f2-47b1-824c-6e96a415fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified data collator\n",
    "def my_collator(batch):\n",
    "    # data collater expects tensors. remove answer and question from batch and collate.\n",
    "    answers = [sample.pop('answer') for sample in batch]\n",
    "    questions = [sample.pop('question') for sample in batch]\n",
    "    collated_batch = data_collator(batch)\n",
    "    # add question and answer back\n",
    "    collated_batch['answer'] = answers\n",
    "    collated_batch['question'] = questions\n",
    "    return collated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88368730-c90a-4f83-866a-f87be2e741e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=train_batch_size, collate_fn=my_collator, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"eval\"], batch_size=eval_batch_size, collate_fn=my_collator, pin_memory=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=test_batch_size, collate_fn=my_collator, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869df7d-fd1c-4beb-b99b-23316f88b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT config - prompt tuning \n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=task_type,\n",
    "    prompt_tuning_init=prompt_tuning_init,\n",
    "    num_virtual_tokens=num_virtual_tokens,\n",
    "    prompt_tuning_init_text=prompt,\n",
    "    tokenizer_name_or_path=base_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f209bba-aa7c-41d3-9460-54c32243211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,480 || all params: 783,170,560 || trainable%: 0.0026150114733628394\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(base_model)\n",
    "model = get_peft_model(model, peft_config).to(device)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b94c5e2c-ca03-4e8b-bf67-d2fe49b191f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 3461 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf6fbf5-f796-4ad5-9477-2480e6153aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Eval Metric\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0421cc0-f84b-4f34-a7a7-75d8a17a69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_dataloader)\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        # if idx == n_batches:\n",
    "        #     break\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k not in ['answer', 'question']}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"train_loss: {loss.item():.5f}\")\n",
    "    epoch_loss = total_loss/num_batches\n",
    "    \n",
    "    # Eval loop\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(device) for k,v in batch.items() if k not in ['answer','labels', 'question']}\n",
    "            outputs = model.generate(**inputs, max_length=max_length_output)\n",
    "            # decode output\n",
    "            predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "            # add new line before each line for rougeL\n",
    "            predictions = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in predictions]\n",
    "            references = [re.sub(r'<<.*?>>', '', ref).replace(\"####\", \"The Answer:\") for ref in batch[\"answer\"]]\n",
    "            rouge_score.add_batch(predictions=predictions, references=references)\n",
    "    \n",
    "    # compute score for the epoch\n",
    "    result = rouge_score.compute()\n",
    "    # display the training loss and RougeLsum - f1 score\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {epoch_loss:.5f}, RougeLsum(F1 score):{result['rougeLsum']}\")\n",
    "\n",
    "\n",
    "# save\n",
    "model.save_pretrained(trained_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395047d-b6f3-4cc8-8bf8-6fc9bb50cf7d",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f537740-7e7c-42ce-8e82-be113ecb29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def saveResponse(filePath, data):\n",
    "    with open(filePath, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerows([[\"Question\", \"Generated\", \"Actual\"]])\n",
    "        csv_writer.writerows(data)\n",
    "    print(f'Response saved to: {filePath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7bb1009-267a-4498-9041-784bfd636150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load trained model \n",
    "tokenizer = T5Tokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load the pre trained model\n",
    "config = PeftConfig.from_pretrained(trained_model_path)\n",
    "pretrained_model = T5ForConditionalGeneration.from_pretrained(config.base_model_name_or_path)\n",
    "pretrained_model = PeftModel.from_pretrained(pretrained_model, trained_model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb26c28-9149-4ef3-8c6b-8cbdd0318cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loop\n",
    "responses = []\n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "    inputs = {k: v.to(device) for k,v in batch.items() if k not in ['answer','labels', 'question']}\n",
    "    outputs = pretrained_model.generate(**inputs, max_length=max_length_output)\n",
    "    predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    questions = batch['question']\n",
    "    answers = batch['answer']\n",
    "    batch_responses = list(zip(questions, predictions, answers))\n",
    "    responses.extend(batch_responses)\n",
    "    if(idx%100 == 0):\n",
    "        print(f\"{idx+1}/{len(test_dataloader)}\")\n",
    "# save responses for test data\n",
    "saveResponse(test_responses_path, responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
